from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import arxiv
import chromadb
import requests
import json
from typing import List, Optional
import uvicorn
from datetime import datetime
from rank_bm25 import BM25Okapi
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import os
import shutil

app = FastAPI(title="ArXiv Search Engine")

# Configuration Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gemma:2b"

# Mod√®les
class SearchQuery(BaseModel):
    query: str
    max_results: int = 10
    category: Optional[str] = None

class Paper(BaseModel):
    id: str
    title: str
    authors: List[str]
    summary: str
    published: str
    categories: List[str]
    score: float = 0.0
    bm25_score: float = 0.0
    semantic_score: float = 0.0

# Base de donn√©es
DB_PATH = "./chroma_db"
papers_cache = []  # Cache pour BM25
bm25 = None
collection = None

def initialize_database():
    """Initialize ChromaDB with correct embedding dimensions"""
    global collection
    
    # Si la base existe avec de mauvaises dimensions, la supprimer
    if os.path.exists(DB_PATH):
        try:
            temp_client = chromadb.PersistentClient(path=DB_PATH)
            try:
                temp_collection = temp_client.get_collection("arxiv_papers")
                # Tester avec un embedding de test
                test_embedding = get_ollama_embedding("test")
                if test_embedding:
                    temp_collection.query(
                        query_embeddings=[test_embedding],
                        n_results=1
                    )
                collection = temp_collection
                print("‚úÖ Base de donn√©es existante compatible")
                return
            except Exception as e:
                if "dimension" in str(e).lower():
                    print("‚ùå Dimensions incompatibles, suppression de l'ancienne base...")
                    temp_client.delete_collection("arxiv_papers")
                    del temp_client
                    shutil.rmtree(DB_PATH)
                else:
                    print(f"Autre erreur: {e}")
        except Exception as e:
            print(f"Erreur lors de la v√©rification: {e}")
    
    # Cr√©er une nouvelle base avec m√©trique cosinus
    client = chromadb.PersistentClient(path=DB_PATH)
    collection = client.create_collection(
        name="arxiv_papers",
        metadata={"hnsw:space": "cosine"}  # Utiliser la m√©trique cosinus
    )
    print("‚úÖ Nouvelle base de donn√©es cr√©√©e avec m√©trique cosinus")

def get_ollama_embedding(text: str):
    """G√©n√®re un embedding avec Ollama/nomic-embed-text"""
    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/embeddings",
            json={"model": EMBED_MODEL, "prompt": text},
            timeout=30
        )
        if response.status_code == 200:
            embedding = response.json()["embedding"]
            print(f"üìè Dimension de l'embedding: {len(embedding)}")
            return embedding
        else:
            print(f"Erreur Ollama embedding: {response.status_code}")
            return None
    except Exception as e:
        print(f"Erreur connexion Ollama: {e}")
        return None

def expand_query_with_gemma(query: str):
    """Expansion de requ√™te avec Gemma via Ollama"""
    prompt = f"""Reformule cette requ√™te de recherche scientifique en ajoutant des termes similaires et synonymes pertinents. Donne SEULEMENT la requ√™te √©tendue, sans formatage ni explication.

Requ√™te: {query}

Requ√™te √©tendue:"""
    
    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": LLM_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 50}
            },
            timeout=30
        )
        if response.status_code == 200:
            expanded = response.json()["response"].strip()
            # Nettoyer la r√©ponse de tout formatage markdown ou pr√©fixe
            expanded = re.sub(r'\*\*.*?\*\*:?\s*', '', expanded)
            expanded = re.sub(r'^["\']|["\']$', '', expanded)
            expanded = expanded.split('\n')[0].strip()  # Prendre seulement la premi√®re ligne
            
            return expanded if len(expanded) > len(query) and len(expanded) < 100 else query
        else:
            return query
    except Exception as e:
        print(f"Erreur expansion de requ√™te: {e}")
        return query

def preprocess_text(text: str):
    """Pr√©processing pour BM25"""
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    return text.split()

def fetch_arxiv_papers(query: str, max_results: int = 50):
    """R√©cup√®re les papiers d'ArXiv"""
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )
    
    papers = []
    for result in search.results():
        paper = {
            'id': result.entry_id.split('/')[-1],
            'title': result.title,
            'authors': [str(author) for author in result.authors],
            'summary': result.summary,
            'published': result.published.isoformat(),
            'categories': result.categories
        }
        papers.append(paper)
    
    return papers

def store_papers(papers):
    """Stocke les papiers dans ChromaDB et met √† jour BM25"""
    global papers_cache, bm25, collection
    
    if not collection:
        initialize_database()
    
    documents = []
    metadatas = []
    ids = []
    
    # Filtrer les papiers d√©j√† existants
    existing_ids = {paper['id'] for paper in papers_cache}
    new_papers = [paper for paper in papers if paper['id'] not in existing_ids]
    
    if not new_papers:
        print("Aucun nouveau papier √† indexer")
        return
    
    for paper in new_papers:
        # Texte combin√© pour l'embedding
        text = f"{paper['title']} {paper['summary']}"
        documents.append(text)
        
        metadatas.append({
            'title': paper['title'],
            'authors': json.dumps(paper['authors']),
            'published': paper['published'],
            'categories': json.dumps(paper['categories'])
        })
        
        ids.append(paper['id'])
        
        # Ajouter au cache pour BM25
        papers_cache.append({
            'id': paper['id'],
            'text': text,
            'metadata': metadatas[-1]
        })
    
    print(f"üîÑ G√©n√©ration des embeddings pour {len(documents)} documents...")
    
    # G√©n√©ration des embeddings avec Ollama
    embeddings = []
    for i, doc in enumerate(documents):
        print(f"üìÑ Processing document {i+1}/{len(documents)}")
        embedding = get_ollama_embedding(doc)
        if embedding:
            embeddings.append(embedding)
        else:
            print(f"‚ùå Erreur embedding pour le document {i+1}")
            return False
    
    try:
        collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        print(f"‚úÖ {len(embeddings)} documents stock√©s avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du stockage: {e}")
        return False
    
    # Reconstruire BM25
    corpus = [preprocess_text(paper['text']) for paper in papers_cache]
    if corpus:
        bm25 = BM25Okapi(corpus)
        print("‚úÖ Index BM25 reconstruit")
    
    return True

def hybrid_search(query: str, max_results: int = 10, category: str = None):
    """Recherche hybride BM25 + s√©mantique"""
    global bm25, papers_cache, collection
    
    if not papers_cache:
        return []
    
    if not collection:
        initialize_database()
    
    # 1. Expansion de requ√™te
    expanded_query = expand_query_with_gemma(query)
    print(f"üîç Requ√™te originale: {query}")
    print(f"üîç Requ√™te √©tendue: {expanded_query}")
    
    # 2. Recherche s√©mantique avec Ollama
    query_embedding = get_ollama_embedding(expanded_query)
    semantic_results = []
    
    if query_embedding:
        try:
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=min(max_results * 2, len(papers_cache))
            )
            semantic_results = results
            print(f"‚úÖ Recherche s√©mantique: {len(results['ids'][0])} r√©sultats")
            
            # Debug: afficher quelques distances pour diagnostic
            if results['distances'][0]:
                distances = results['distances'][0][:5]  # Les 5 premi√®res
                print(f"üîç Distances s√©mantiques (√©chantillon): {[f'{d:.4f}' for d in distances]}")
                
        except Exception as e:
            print(f"‚ùå Erreur recherche s√©mantique: {e}")
    
    # 3. Recherche BM25
    bm25_scores = []
    if bm25:
        query_tokens = preprocess_text(expanded_query)
        bm25_scores = bm25.get_scores(query_tokens)
        print(f"‚úÖ Recherche BM25: {len(bm25_scores)} scores calcul√©s")
    
    # 4. Fusion des scores
    papers = []
    semantic_ids = semantic_results.get('ids', [[]])[0] if semantic_results else []
    
    # Normalisation BM25 s√©curis√©e - CORRECTION DE L'ERREUR ICI
    if len(bm25_scores) > 0:
        # Convertir en numpy array et calculer le max
        bm25_array = np.array(bm25_scores)
        max_bm25 = float(np.max(bm25_array)) if bm25_array.size > 0 else 0.0
    else:
        max_bm25 = 0.0
    
    print(f"üìä Max BM25 score: {max_bm25}")
    
    for i, paper in enumerate(papers_cache):
        # Score BM25 (normalis√©)
        bm25_score = float(bm25_scores[i]) if i < len(bm25_scores) else 0.0
        bm25_normalized = bm25_score / (max_bm25 + 1e-6) if max_bm25 > 0 else 0.0
        
        # Score s√©mantique - Correction pour ChromaDB
        semantic_score = 0.0
        if paper['id'] in semantic_ids:
            idx = semantic_ids.index(paper['id'])
            distance = float(semantic_results['distances'][0][idx])
            
            # ChromaDB retourne une distance cosinus (0 = identique, 1 = oppos√©)
            # On convertit en score de similarit√© (1 = identique, 0 = oppos√©)
            semantic_score = max(0.0, 1.0 - distance)
            
            # Debug: afficher quelques scores pour v√©rification
            if i < 3:  # Afficher les 3 premiers
                print(f"üîç Paper {i+1}: distance={distance:.4f}, score={semantic_score:.4f}")
        
        # Score hybride (pond√©ration 50/50)
        hybrid_score = 0.5 * bm25_normalized + 0.5 * semantic_score
        
        # Filtrage par cat√©gorie
        if category:
            categories = json.loads(paper['metadata']['categories'])
            if category not in categories:
                continue
        
        # R√©cup√©rer le r√©sum√© original
        summary = paper['text']
        if paper['metadata']['title'] in summary:
            summary = summary.split(paper['metadata']['title'], 1)[1].strip()
        
        paper_obj = Paper(
            id=paper['id'],
            title=paper['metadata']['title'],
            authors=json.loads(paper['metadata']['authors']),
            summary=summary[:500] + "..." if len(summary) > 500 else summary,
            published=paper['metadata']['published'],
            categories=json.loads(paper['metadata']['categories']),
            score=float(hybrid_score),
            bm25_score=float(bm25_normalized),
            semantic_score=float(semantic_score)
        )
        papers.append(paper_obj)
    
    # Tri par score hybride
    papers.sort(key=lambda x: x.score, reverse=True)
    return papers[:max_results]

@app.on_event("startup")
async def startup_event():
    """Initialize database on startup"""
    initialize_database()

@app.get("/")
async def root():
    return {"message": "ArXiv Search Engine API avec Ollama"}

@app.post("/reset")
async def reset_database():
    """Remet √† z√©ro la base de donn√©es"""
    global papers_cache, bm25, collection
    
    try:
        if os.path.exists(DB_PATH):
            shutil.rmtree(DB_PATH)
        
        papers_cache = []
        bm25 = None
        collection = None
        
        initialize_database()
        
        return {"message": "Base de donn√©es r√©initialis√©e avec succ√®s"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/index")
async def index_papers(query: str = "machine learning", max_results: int = 20):
    """Indexe de nouveaux papiers d'ArXiv"""
    try:
        print(f"üîç Recherche ArXiv: '{query}' (max: {max_results})")
        papers = fetch_arxiv_papers(query, max_results)
        print(f"üìö {len(papers)} papiers r√©cup√©r√©s d'ArXiv")
        
        success = store_papers(papers)
        if success:
            return {"message": f"Index√© {len(papers)} papiers avec Ollama embeddings"}
        else:
            raise HTTPException(status_code=500, detail="Erreur lors de l'indexation")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", response_model=List[Paper])
async def search_papers(query: SearchQuery):
    """Recherche hybride BM25 + s√©mantique"""
    try:
        papers = hybrid_search(
            query.query, 
            query.max_results, 
            query.category
        )
        print(f"üìä R√©sultats: {len(papers)} papiers trouv√©s")
        return papers
    except Exception as e:
        print(f"‚ùå Erreur recherche: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """Statistiques de la base de donn√©es"""
    try:
        count = len(papers_cache)
        
        # Test de connexion Ollama
        ollama_status = "disconnected"
        try:
            test_response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
            if test_response.status_code == 200:
                ollama_status = "connected"
        except:
            pass
        
        return {
            "total_papers": count,
            "bm25_ready": bm25 is not None,
            "ollama_status": ollama_status,
            "collection_initialized": collection is not None
        }
    except Exception as e:
        return {"error": str(e), "total_papers": 0}

if __name__ == "__main__":
    print("üöÄ D√©marrage du moteur de recherche ArXiv avec Ollama")
    print("üìã V√©rifiez qu'Ollama est d√©marr√© avec les mod√®les:")
    print("   - ollama pull nomic-embed-text")
    print("   - ollama pull gemma:2b")
    print("")
    print("üîß Endpoints disponibles:")
    print("   - POST /reset : R√©initialiser la base")
    print("   - POST /index : Indexer des papiers")
    print("   - POST /search : Rechercher")
    print("   - GET /stats : Statistiques")
    uvicorn.run(app, host="0.0.0.0", port=8000)